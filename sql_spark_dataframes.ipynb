{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "name": "A4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "tags": [
          "instructions"
        ],
        "id": "L40eXcn6C1GU"
      },
      "source": [
        "## Data Intensive Distributed Computing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "tags": [
          "setup"
        ],
        "id": "NWzWM_87C1Gb"
      },
      "source": [
        "!apt-get update -qq > /dev/null\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://downloads.apache.org/spark/spark-2.4.7/spark-2.4.7-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.7-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "!wget -q https://student.cs.uwaterloo.ca/~cs451/content/cs431/sql-data.tgz\n",
        "!tar -xzf sql-data.tgz"
      ],
      "execution_count": 220,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "tags": [
          "instructions"
        ],
        "id": "FT4V7xlCC1Gc"
      },
      "source": [
        "Next, we launch Spark.  When I used the RDD interface for previous assignments, I created a `SparkContext` when I launched Spark.   To use Spark SQL and the DataFrame interface, I instead create a `SparkSession`.   I do that as shown in the next cell (run it!).    "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "tags": [
          "setup"
        ],
        "id": "NyH7WzP0C1Gc"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.7-bin-hadoop2.7\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "import random\n",
        "\n",
        "spark = SparkSession.builder.appName(\"IrTest\").master(\"local[2]\").config('spark.ui.port', random.randrange(4000,5000)).getOrCreate()"
      ],
      "execution_count": 221,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "tags": [
          "instructions"
        ],
        "id": "TM53t79tC1Gd"
      },
      "source": [
        "Next, let's create DataFrames from the TPC-H data files.  We have already downloaded the TPC-H data files.\n",
        "\n",
        "There is one file for each table in the TPC-H database, e.g., `nation.tbl` for the TPC-H Nation table, `customer.tbl` for the TPC-H Customer table, and so on.    These are plain text csv files, with the character `|` used as a field separator.\n",
        "\n",
        "Create a Spark DataFrame corresponding to the TPC-H Nation table by loading the data from the `nation.tbl` file.   Run the code in the next cell to do this.   After I run this code, `nation_raw` will refer to Ir new Spark DataFrame.   The Spark `show()` method will display a small (default 20) number of elements from the DataFrame, so that I can inspect them. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "tags": [
          "test"
        ],
        "id": "RWktEcnhC1Gd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13b667a9-7005-4880-ddb0-2261298ccb1a"
      },
      "source": [
        "nation_raw = spark.read.csv(\"nation.tbl\",sep='|',inferSchema=True)\n",
        "nation_raw.show()"
      ],
      "execution_count": 222,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+----------+---+--------------------+----+\n",
            "|_c0|       _c1|_c2|                 _c3| _c4|\n",
            "+---+----------+---+--------------------+----+\n",
            "|  0|   ALGERIA|  0| haggle. carefull...|null|\n",
            "|  1| ARGENTINA|  1|al foxes promise ...|null|\n",
            "|  2|    BRAZIL|  1|y alongside of th...|null|\n",
            "|  3|    CANADA|  1|eas hang ironic, ...|null|\n",
            "|  4|     EGYPT|  4|y above the caref...|null|\n",
            "|  5|  ETHIOPIA|  0|ven packages wake...|null|\n",
            "|  6|    FRANCE|  3|refully final req...|null|\n",
            "|  7|   GERMANY|  3|l platelets. regu...|null|\n",
            "|  8|     INDIA|  2|ss excuses cajole...|null|\n",
            "|  9| INDONESIA|  2| slyly express as...|null|\n",
            "| 10|      IRAN|  4|efully alongside ...|null|\n",
            "| 11|      IRAQ|  4|nic deposits boos...|null|\n",
            "| 12|     JAPAN|  2|ously. final, exp...|null|\n",
            "| 13|    JORDAN|  4|ic deposits are b...|null|\n",
            "| 14|     KENYA|  0| pending excuses ...|null|\n",
            "| 15|   MOROCCO|  0|rns. blithely bol...|null|\n",
            "| 16|MOZAMBIQUE|  0|s. ironic, unusua...|null|\n",
            "| 17|      PERU|  1|platelets. blithe...|null|\n",
            "| 18|     CHINA|  2|c dependencies. f...|null|\n",
            "| 19|   ROMANIA|  3|ular asymptotes a...|null|\n",
            "+---+----------+---+--------------------+----+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "tags": [
          "instructions"
        ],
        "id": "up0Hcd4DC1Ge"
      },
      "source": [
        "Now I have a DataFrame to work with.   The columns of the DataFrame correspond to the fields of the TPC-H Nation table, so have a look at the TPC-H schema diagram to see what I are dealing with.   Column c0 is the NATIONKEY, column c1 is the NAME, c2 is the REGIONKEY, and so on.   Since this is a synthetic database, I'll notice that the data in some of the fields (like the COMMENT field) consists of random words.   That's fine.   I can also ask Spark to tell I about the type of data in each column:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "tags": [
          "test"
        ],
        "id": "Y6VMnplEC1Ge",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d1124b4-71a5-4e4e-fa11-3ebfda07b10f"
      },
      "source": [
        "nation_raw.dtypes"
      ],
      "execution_count": 223,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('_c0', 'int'),\n",
              " ('_c1', 'string'),\n",
              " ('_c2', 'int'),\n",
              " ('_c3', 'string'),\n",
              " ('_c4', 'string')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 223
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "instructions"
        ],
        "id": "cMImZgYDC1Gf"
      },
      "source": [
        "Before going on, let's clean this DataFrame up a bit, to make it easier to use.   First, let's assign names to the columns, so that we can remember what information each column holds.   Second, I'll notice that Spark has created an extra final column (filled with `null` values) because each line in the input file ends with a separator character (|).  Let's drop that final column, since we don't need it.   Run the following code to do this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "tags": [
          "test"
        ],
        "id": "OXSkfqqtC1Gf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72c1317c-cbed-4347-b8ff-a3a572ad63b2"
      },
      "source": [
        "nation = nation_raw.toDF('NationKey','Name','RegionKey','Comment','extra').drop('extra').cache()\n",
        "nation.show()"
      ],
      "execution_count": 224,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+----------+---------+--------------------+\n",
            "|NationKey|      Name|RegionKey|             Comment|\n",
            "+---------+----------+---------+--------------------+\n",
            "|        0|   ALGERIA|        0| haggle. carefull...|\n",
            "|        1| ARGENTINA|        1|al foxes promise ...|\n",
            "|        2|    BRAZIL|        1|y alongside of th...|\n",
            "|        3|    CANADA|        1|eas hang ironic, ...|\n",
            "|        4|     EGYPT|        4|y above the caref...|\n",
            "|        5|  ETHIOPIA|        0|ven packages wake...|\n",
            "|        6|    FRANCE|        3|refully final req...|\n",
            "|        7|   GERMANY|        3|l platelets. regu...|\n",
            "|        8|     INDIA|        2|ss excuses cajole...|\n",
            "|        9| INDONESIA|        2| slyly express as...|\n",
            "|       10|      IRAN|        4|efully alongside ...|\n",
            "|       11|      IRAQ|        4|nic deposits boos...|\n",
            "|       12|     JAPAN|        2|ously. final, exp...|\n",
            "|       13|    JORDAN|        4|ic deposits are b...|\n",
            "|       14|     KENYA|        0| pending excuses ...|\n",
            "|       15|   MOROCCO|        0|rns. blithely bol...|\n",
            "|       16|MOZAMBIQUE|        0|s. ironic, unusua...|\n",
            "|       17|      PERU|        1|platelets. blithe...|\n",
            "|       18|     CHINA|        2|c dependencies. f...|\n",
            "|       19|   ROMANIA|        3|ular asymptotes a...|\n",
            "+---------+----------+---------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "tags": [
          "instructions"
        ],
        "id": "xZbbMWnnC1Gg"
      },
      "source": [
        "This style of code should look familar to I.  We started with the `nation_raw` DataFrame and applied a series of DataFrame operations (`toDF`, `drop`, and `cache`).   This is just like the RDD interface, except now we are applying DataFrame operations to DataFrames, instead of RDD operations to RDDs.\n",
        "\n",
        "Next, let's load up the TPC-H Supplier table, and then try performing some queries:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "tags": [
          "test"
        ],
        "id": "N_0dLU6mC1Gg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a894568c-ebae-41ee-dd4c-45f7ce18d0ea"
      },
      "source": [
        "supplier_raw = spark.read.csv(\"supplier.tbl\",sep='|',inferSchema=True).drop(\"_c7\")\n",
        "supplier = supplier_raw.toDF(\"SuppKey\",\"Name\",\"Address\",\"NationKey\",\"Phone\",\"AcctBal\",\"Comment\").cache()\n",
        "supplier.show()"
      ],
      "execution_count": 225,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+------------------+--------------------+---------+---------------+-------+--------------------+\n",
            "|SuppKey|              Name|             Address|NationKey|          Phone|AcctBal|             Comment|\n",
            "+-------+------------------+--------------------+---------+---------------+-------+--------------------+\n",
            "|      1|Supplier#000000001| N kD4on9OM Ipw3,...|       17|27-918-335-1736|5755.94|each slyly above ...|\n",
            "|      2|Supplier#000000002|89eJ5ksX3ImxJQBvx...|        5|15-679-861-2259|4032.68| slyly bold instr...|\n",
            "|      3|Supplier#000000003|q1,G3Pj6OjIuUYfUo...|        1|11-383-516-1199| 4192.4|blithely silent r...|\n",
            "|      4|Supplier#000000004|Bk7ah4CK8SYQTepEm...|       15|25-843-787-7479|4641.08|riously even requ...|\n",
            "|      5|Supplier#000000005|   Gcdm2rJRzl5qlTVzc|       11|21-151-690-3663|-283.84|. slyly regular p...|\n",
            "|      6|Supplier#000000006|        tQxuVm7s7CnK|       14|24-696-997-4969|1365.79|final accounts. r...|\n",
            "|      7|Supplier#000000007|s,4TicNGB4uO6PaSq...|       23|33-990-965-2201|6820.35|s unwind silently...|\n",
            "|      8|Supplier#000000008|9Sq4bBH2FQEmaFOoc...|       17|27-498-742-3860|7627.85|al pinto beans. a...|\n",
            "|      9|Supplier#000000009|1KhUgZegwM3ua7dsY...|       10|20-403-398-8662|5302.37|s. unusual, even ...|\n",
            "|     10|Supplier#000000010|  Saygah3gYWMp72i PY|       24|34-852-489-8585|3891.91|ing waters. regul...|\n",
            "|     11|Supplier#000000011|    JfwTs,LZrV, M,9C|       18|28-613-996-1505|3393.08|y ironic packages...|\n",
            "|     12|Supplier#000000012|         aLIW  q0HYd|        8|18-179-925-7181|1432.69|al packages nag a...|\n",
            "|     13|Supplier#000000013|HK71HQyWoqRWOX8GI...|        3|13-727-620-7813|9107.22|requests engage r...|\n",
            "|     14|Supplier#000000014|     EXsnO5pTNj4iZRm|       15|25-656-247-5058|9189.82|l accounts boost....|\n",
            "|     15|Supplier#000000015|olXVbNBfVzRqgokr1...|        8|18-453-357-6394| 308.56| across the furio...|\n",
            "|     16|Supplier#000000016|YjP5C55zHDXL7LalK...|       22|32-822-502-4215|2972.26|ously express ide...|\n",
            "|     17|Supplier#000000017|c2d,ESHRSkK3WYnxp...|       19|29-601-884-9219|1687.81|eep against the f...|\n",
            "|     18|Supplier#000000018|    PGGVE5PWAMwKDZw |       16|26-729-551-1115|7040.82|accounts snooze s...|\n",
            "|     19|Supplier#000000019|edZT3es,nBFD8lBXT...|       24|34-278-310-2731|6150.38|refully final fox...|\n",
            "|     20|Supplier#000000020|iybAE,RmTymrZVYaF...|        3|13-715-945-6730| 530.82|n, ironic ideas w...|\n",
            "+-------+------------------+--------------------+---------+---------------+-------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "tags": [
          "instructions"
        ],
        "id": "KF0R8aWdC1Gh"
      },
      "source": [
        "---\n",
        "#### Writing Queries\n",
        "There are two equivalent ways of writing queries over Spark DataFrames.   The first way is to assign a \"view name\" to the DataFrame, and then write SQL queries referring to those view names using the `sql` operation.  \n",
        "\n",
        "The code below gives the view names \"nation\" and \"supplier\" to the two DataFrames we've already created."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "tags": [
          "test"
        ],
        "id": "-cDsMeG9C1Gh"
      },
      "source": [
        "supplier.createOrReplaceTempView(\"supplier\")\n",
        "nation.createOrReplaceTempView(\"nation\")"
      ],
      "execution_count": 226,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "tags": [
          "instructions"
        ],
        "id": "QudYzGNFC1Gh"
      },
      "source": [
        "Now, we can write SQL queries that refer to the \"supplier\" and \"nation\" views as tables.   For example, suppose we want to see the names and addresses of suppliers who have account balances above 9900.00:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "tags": [
          "test"
        ],
        "id": "-H4sAftlC1Gi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c031f2c4-178f-40d8-d1b6-97945d1714bf"
      },
      "source": [
        "q1_result = spark.sql(\"select Name, Address, AcctBal from supplier where AcctBal > 9900.00\")\n",
        "q1_result.show()"
      ],
      "execution_count": 227,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------------+--------------------+-------+\n",
            "|              Name|             Address|AcctBal|\n",
            "+------------------+--------------------+-------+\n",
            "|Supplier#000000049|     Nvq 6macF4GtJvz|9915.24|\n",
            "|Supplier#000000234|iMrk7HUD87at3IIh4rBi| 9957.0|\n",
            "|Supplier#000000693|S,mnHfsroFOVieQGd...|9956.55|\n",
            "|Supplier#000000855|ekQwhb9fh5VGIvMBJ...|9964.88|\n",
            "|Supplier#000000892|j6prA4M3sX9a9xHem...|9993.46|\n",
            "+------------------+--------------------+-------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "tags": [
          "instructions"
        ],
        "id": "YzNdZp56C1Gi"
      },
      "source": [
        "In the example above, the `sql` command runs the SQL query against the supplier table.   It returns the query result as a new DataFrame, which `q1_result` refers to.\n",
        "\n",
        "Instead of writing Ir queries in SQL and running them using `sql`, it is possible to do the same thing by applying a sequence of DataFrame operations to the input DataFrames, as I did when I were using the RDD interface in the previous assignments.    For example, to answer the same query that we just answered using SQL, we can do the following:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "tags": [
          "test"
        ],
        "id": "dCBfPiHwC1Gi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86597d73-a9b3-4f5b-a938-0a4979ade1a7"
      },
      "source": [
        "q1_resultB = supplier.filter(\"AcctBal > 9900.00\").select('Name','Address','AcctBal')\n",
        "q1_resultB.show()"
      ],
      "execution_count": 228,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------------+--------------------+-------+\n",
            "|              Name|             Address|AcctBal|\n",
            "+------------------+--------------------+-------+\n",
            "|Supplier#000000049|     Nvq 6macF4GtJvz|9915.24|\n",
            "|Supplier#000000234|iMrk7HUD87at3IIh4rBi| 9957.0|\n",
            "|Supplier#000000693|S,mnHfsroFOVieQGd...|9956.55|\n",
            "|Supplier#000000855|ekQwhb9fh5VGIvMBJ...|9964.88|\n",
            "|Supplier#000000892|j6prA4M3sX9a9xHem...|9993.46|\n",
            "+------------------+--------------------+-------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "tags": [
          "setup",
          "global",
          "code"
        ],
        "id": "51r8AkHXC1Gk"
      },
      "source": [
        "def load_dataset_and_set_views():\n",
        "    global supplier, orders, customer, partsupp, nation, part\n",
        "    \n",
        "    supplier_raw = spark.read.csv(\"supplier.tbl\",sep='|',inferSchema=True).drop(\"_c7\")\n",
        "    supplier = supplier_raw.toDF(\"SuppKey\",\"Name\",\"Address\",\"NationKey\",\"Phone\",\"AcctBal\",\"Comment\").cache()\n",
        "    supplier.createOrReplaceTempView(\"supplier\")\n",
        "    \n",
        "    # Ir solution to Question 0 here\n",
        "    order_raw = spark.read.csv(\"orders.tbl\",sep='|',inferSchema=True).drop(\"_c9\")\n",
        "    orders = order_raw.toDF(\"OrderKey\", \"CustKey\", \"OrderStatus\", \"TotalPrice\", \"OrderDate\", \"OrderPriority\", \"Clerk\", \"ShipPriority\", \"Comment\").cache()\n",
        "    orders.createOrReplaceTempView(\"orders\")\n",
        "\n",
        "    customer_raw = spark.read.csv(\"customer.tbl\",sep='|',inferSchema=True).drop(\"_c8\")\n",
        "    customer = customer_raw.toDF(\"CustKey\",\"Name\", \"Address\", \"NationKey\", \"Phone\",\"AccBal\",\"MktSegment\", \"Comment\").cache()\n",
        "    customer.createOrReplaceTempView(\"customer\")\n",
        "    \n",
        "    partsupp_raw = spark.read.csv(\"partsupp.tbl\",sep='|',inferSchema=True).drop(\"_c5\")\n",
        "    partsupp = partsupp_raw.toDF(\"PartKey\", \"SuppKey\", \"AvailQty\", \"SupplyCost\", \"Comment\").cache()\n",
        "    partsupp.createOrReplaceTempView(\"partsupp\")\n",
        "    \n",
        "    nation_raw = spark.read.csv(\"nation.tbl\",sep='|',inferSchema=True).drop(\"_c4\")\n",
        "    nation = nation_raw.toDF(\"NationKey\", \"Name\", \"RegionKey\", \"Comment\").cache()\n",
        "    nation.createOrReplaceTempView(\"nation\")\n",
        "    \n",
        "    part_raw = spark.read.csv(\"part.tbl\",sep='|',inferSchema=True).drop(\"_c9\")\n",
        "    part = part_raw.toDF(\"PartKey\", \"Name\", \"Mfgr\", \"Brand\", \"Type\", \"Size\", \"Container\", \"RetailPrice\", \"Comment\").cache()\n",
        "    part.createOrReplaceTempView(\"part\")"
      ],
      "execution_count": 229,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "tags": [
          "test",
          "setup"
        ],
        "id": "9aStOLWEC1Gk"
      },
      "source": [
        "# Ir tests here\n",
        "load_dataset_and_set_views()"
      ],
      "execution_count": 230,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "tags": [
          "code"
        ],
        "id": "6LgeOpebC1Gl"
      },
      "source": [
        "def five_highest_totalprice_orders_sql():\n",
        "    # Ir solution to Question 1 here\n",
        "    query = \"select OrderKey, OrderDate, TotalPrice from orders order by TotalPrice desc limit 5\"\n",
        "    return spark.sql(query)\n"
      ],
      "execution_count": 231,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "tags": [
          "test"
        ],
        "id": "w5_b0dErC1Gl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77145c86-2a6b-498a-8771-054dd61478f4"
      },
      "source": [
        "# Ir tests here\n",
        "five_highest_totalprice_orders_sql().show()"
      ],
      "execution_count": 232,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------+-------------------+----------+\n",
            "|OrderKey|          OrderDate|TotalPrice|\n",
            "+--------+-------------------+----------+\n",
            "|  279812|1994-02-19 00:00:00| 479129.21|\n",
            "|  370726|1996-09-29 00:00:00|  460099.4|\n",
            "|   66659|1993-10-15 00:00:00| 458396.42|\n",
            "|  253639|1998-01-23 00:00:00| 456532.89|\n",
            "|  502886|1994-04-12 00:00:00| 456423.88|\n",
            "+--------+-------------------+----------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "tags": [
          "code"
        ],
        "id": "530hPHAaC1Gm"
      },
      "source": [
        "def five_highest_totalprice_orders_dtf():\n",
        "    # Ir solution to Question 2 here\n",
        "    from pyspark.sql.functions import desc\n",
        "    return orders.orderBy(desc('TotalPrice')).select('OrderKey', 'OrderDate', 'TotalPrice').limit(5)"
      ],
      "execution_count": 234,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "tags": [
          "test"
        ],
        "id": "QxuYASvlC1Gm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32605824-aac0-45a0-c1cd-d97aeab80780"
      },
      "source": [
        "# Ir tests here\n",
        "five_highest_totalprice_orders_dtf().show(5)"
      ],
      "execution_count": 235,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------+-------------------+----------+\n",
            "|OrderKey|          OrderDate|TotalPrice|\n",
            "+--------+-------------------+----------+\n",
            "|  279812|1994-02-19 00:00:00| 479129.21|\n",
            "|  370726|1996-09-29 00:00:00|  460099.4|\n",
            "|   66659|1993-10-15 00:00:00| 458396.42|\n",
            "|  253639|1998-01-23 00:00:00| 456532.89|\n",
            "|  502886|1994-04-12 00:00:00| 456423.88|\n",
            "+--------+-------------------+----------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "tags": [
          "code"
        ],
        "id": "_VF49h0kC1Gn"
      },
      "source": [
        "def cust_most_recent_order_sql(custkey):\n",
        "    # Ir solution to Question 3 here\n",
        "    query = \"select c.Name, o.OrderDate, o.TotalPrice from customer as c inner join orders as o on o.CustKey = c.CustKey where c.CustKey = {} order by OrderDate desc limit 1\".format(custkey)\n",
        "    return spark.sql(query)"
      ],
      "execution_count": 236,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "tags": [
          "test"
        ],
        "id": "Pc3dX8DqC1Gn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f53f058-3d48-457e-e23f-29d62808dad8"
      },
      "source": [
        "# Ir tests here\n",
        "cust_most_recent_order_sql(1).show()"
      ],
      "execution_count": 237,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------------+-------------------+----------+\n",
            "|              Name|          OrderDate|TotalPrice|\n",
            "+------------------+-------------------+----------+\n",
            "|Customer#000000001|1997-03-04 00:00:00| 268835.44|\n",
            "+------------------+-------------------+----------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "tags": [
          "code"
        ],
        "id": "qAMV9vAoC1Gn"
      },
      "source": [
        "def cust_most_recent_order_dtf(custkey):\n",
        "    # Ir solution to Question 4 here\n",
        "    from pyspark.sql.functions import desc\n",
        "    out_df = customer.filter('CustKey == {}'.format(custkey)).join(orders, orders.CustKey == customer.CustKey, \"inner\").orderBy(desc('OrderDate')).select(\"Name\", \"OrderDate\", \"TotalPrice\")\n",
        "\n",
        "    return out_df.limit(1)"
      ],
      "execution_count": 238,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "tags": [
          "test"
        ],
        "id": "yaUrQINMC1Go",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "215401b2-4559-4daf-f455-fbec838fde57"
      },
      "source": [
        "# Ir tests here\n",
        "cust_most_recent_order_dtf(1).show(10)"
      ],
      "execution_count": 239,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------------+-------------------+----------+\n",
            "|              Name|          OrderDate|TotalPrice|\n",
            "+------------------+-------------------+----------+\n",
            "|Customer#000000001|1997-03-04 00:00:00| 268835.44|\n",
            "+------------------+-------------------+----------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "tags": [
          "code"
        ],
        "id": "hp4mUYHxC1Go"
      },
      "source": [
        "def distinct_supplied_parts(nname):\n",
        "    # Ir solution to Question 5 here\n",
        "    query = \"select count(distinct(ps.PartKey)) from partsupp as ps inner join supplier as s on ps.SuppKey = s.SuppKey inner join nation as n on n.NationKey == s.NationKey where n.Name = '{}'\".format(nname)\n",
        "    #query = \"select n.NationKey, n.Name from supplier as s inner join nation as n on n.NationKey = s.NationKey\"\n",
        "    return spark.sql(query).collect()[0][0]"
      ],
      "execution_count": 240,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "tags": [
          "test"
        ],
        "id": "DZED9ggQC1Go",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d8ad463-db7f-41f6-e77c-35fd425a47f3"
      },
      "source": [
        "# Ir tests here\n",
        "distinct_supplied_parts(\"CANADA\")"
      ],
      "execution_count": 241,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2799"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 241
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "tags": [
          "code"
        ],
        "id": "nz6WzWX8C1Gp"
      },
      "source": [
        "def count_suppliers_brand_per_nation(bname):\n",
        "    # Ir solution to Question 6 here\n",
        "    subquery = \"select distinct(ps.SuppKey) from part as p inner join partsupp as ps on ps.PartKey = p.PartKey where p.Brand = '{}'\".format(bname)\n",
        "    outer_query = \"select n.Name, count(distinct(s.SuppKey)) as Suppliers from supplier as s inner join nation as n on s.NationKey = n.NationKey where s.SuppKey in ({}) group by n.Name order by n.Name\".format(subquery)\n",
        "    return spark.sql(outer_query)"
      ],
      "execution_count": 301,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "tags": [
          "test"
        ],
        "id": "d5-gACGvC1Gp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df279644-fb5a-4c2d-c17e-10252f919ee4"
      },
      "source": [
        "# Ir tests here\n",
        "count_suppliers_brand_per_nation(\"Brand#14\").show()"
      ],
      "execution_count": 302,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+---------+\n",
            "|      Name|Suppliers|\n",
            "+----------+---------+\n",
            "|   ALGERIA|       34|\n",
            "| ARGENTINA|       38|\n",
            "|    BRAZIL|       41|\n",
            "|    CANADA|       35|\n",
            "|     CHINA|       51|\n",
            "|     EGYPT|       39|\n",
            "|  ETHIOPIA|       32|\n",
            "|    FRANCE|       35|\n",
            "|   GERMANY|       49|\n",
            "|     INDIA|       45|\n",
            "| INDONESIA|       45|\n",
            "|      IRAN|       39|\n",
            "|      IRAQ|       40|\n",
            "|     JAPAN|       40|\n",
            "|    JORDAN|       28|\n",
            "|     KENYA|       35|\n",
            "|   MOROCCO|       40|\n",
            "|MOZAMBIQUE|       32|\n",
            "|      PERU|       37|\n",
            "|   ROMANIA|       32|\n",
            "+----------+---------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "tags": [
          "code"
        ],
        "id": "e3J8Mt4JC1Gq"
      },
      "source": [
        "def order_number_per_customer_nation(nname):\n",
        "    # Ir solution to Question 7 here\n",
        "    query = \"select year(o.OrderDate) as Year, count(o.OrderKey) as Orders from nation as n inner join customer as c on c.NationKey = n.NationKey inner join orders as o on o.CustKey = c.CustKey where n.Name = '{}' group by year(o.OrderDate) order by count(o.OrderKey) desc\".format(nname)\n",
        "    \n",
        "    return spark.sql(query)\n"
      ],
      "execution_count": 299,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "tags": [
          "test"
        ],
        "id": "3wrZsYDuC1Gq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9e177ed-bd3a-48f8-e187-5c835cfc44a7"
      },
      "source": [
        "# Ir tests here\n",
        "order_number_per_customer_nation(\"CANADA\").show()"
      ],
      "execution_count": 300,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----+------+\n",
            "|Year|Orders|\n",
            "+----+------+\n",
            "|1992|   982|\n",
            "|1996|   940|\n",
            "|1995|   932|\n",
            "|1997|   921|\n",
            "|1994|   912|\n",
            "|1993|   900|\n",
            "|1998|   595|\n",
            "+----+------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}