{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3-final"
    },
    "colab": {
      "name": "A1.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLRqn3y2NfJi"
      },
      "source": [
        "## CS431/631 Big Data Infrastructure\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3DCzT-rNfJm"
      },
      "source": [
        "---\n",
        "#### Overview of Pointwise Mutual Information\n",
        "\n",
        "If two events $x$ and $y$ are independent, their PMI will be zero.   A positive PMI indicates that $x$ and $y$ are more likely to co-occur than they would be if they were independent.   Similarly, a negative PMI indicates that $x$ and $y$ are less likely to co-occur.   The PMI of events $x$ and $y$ is given by\n",
        "\\begin{equation*}\n",
        "PMI(x,y) = \\log\\frac{p(x,y)}{p(x)p(y)}\n",
        "\\end{equation*}\n",
        "where $p(x)$ and $p(y)$ are the probabilities of occurrence of events $x$ and $y$, and $p(x,y)$ is the probability of co-occurrence of $x$ and $y$.\n",
        "\n",
        "The \"events\" that we are interested in are occurrences of tokens on lines of text in the input file.   For example, one event\n",
        "might represent the occurence of the token \"fire\" a line of text, and another might represent the occurrence of the token \"peace\".   In that case, $p(fire)$ represents the probability that \"fire\" will occur on a line of text, and $p(fire,peace)$ represents the probability that *both* \"fire\" and \"peace\" will occur on the *same* line.   For the purposes of these PMI computations, it does not matter how many times a given token occures on a single line.   Either a line contains a particular token (at least once), or it does not.   For example, consider this line of text:\n",
        "\n",
        "> three three three, said thrice\n",
        "\n",
        "For this line, the following token-pair events have occurred:\n",
        "- (three, said)\n",
        "- (three, thrice)\n",
        "- (said, three)\n",
        "- (said, thrice)\n",
        "- (thrice, three)\n",
        "- (thrice, said)\n",
        "\n",
        "Note that we are not interested in \"reflexive\" pairs, such as (thrice,thrice).\n",
        "\n",
        "In addition to the probabilities of events, we will also be interested in the absolute *number* of occurences of particular events, e.g., the number of lines in which \"fire\" occurs.   We will use $n(x)$ to represent the these numbers.\n",
        "\n",
        "\n",
        "Based this analysis, we want to be able to answer two types of queries:\n",
        "\n",
        "* Two-Token Queries: Given a pair of tokens, $x$ and $y$, report the number of lines on which that pair co-occurs ($n(x,y)$) as well as $PMI(x,y)$.\n",
        "* One-Token Queries: Given a single token, $x$, report the number of lines on which that token occurs ($n(x)$).   In addition, report the five tokens that have the largest PMI with respect to $x$ (and their PMIs).   That is, report the five $y$'s for which $PMI(x,y)$ is largest.\n",
        "\n",
        "To avoid reporting spurious results for the one-token queries, we are only interested in token pairs that co-occur a sufficient number of times.   Therefore, we will use a *threshold* parameter for one-token queries.   A one-token query should only report pairs of tokens that co-occur at least *threshold* times in the input.   For example, given the threshold 12, a one-token query for \"fire\" the should report the five tokens that have the largest PMI (with respect to \"fire\") among all tokens that co-occur with \"fire\" on at least 12 lines.   If there are fewer than five such tokens, report fewer than five.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcXDTtuqENqF"
      },
      "source": [
        "!wget -q https://student.cs.uwaterloo.ca/~cs451/content/cs431/Shakespeare.txt -P C:\\Users\\shah_\\Desktop\n",
        "!wget -q https://student.cs.uwaterloo.ca/~cs451/content/cs431/simple_tokenize.py"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "AWP7tAfsNfJp",
        "tags": []
      },
      "source": [
        "# this imports the SimpleTokenize function from the simple_tokenize.py file that you uploaded\n",
        "from simple_tokenize import simple_tokenize\n",
        "import itertools\n",
        "\n",
        "tokens = []\n",
        "pair_list= []\n",
        "\n",
        "FILE = \"Shakespeare.txt\"\n",
        "TEST_FILE = \"tmp.txt\"\n",
        "# Now, let's tokenize Shakespeare's plays\n",
        "with open(TEST_FILE) as f:\n",
        "    for line in f:\n",
        "        # tokenize, one line at a time\n",
        "        t = list(set(simple_tokenize(line)))\n",
        "        pair_list += list(itertools.permutations(t, 2))\n",
        "        tokens+=t\n",
        "\n",
        "# extend this code to answer Question 1.\n",
        "# when your code is executed, it should print the number of distinct tokens and the number of distinct token pairs\n",
        "\n",
        "# get unique tokens\n",
        "unique_tokens = list(set(tokens))\n",
        "pair_list_set = list(set(pair_list))\n",
        "\n",
        "print(\"Distinct Tokens:\", len(unique_tokens))\n",
        "print(\"Distinct Pairs:\", len(pair_list_set))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Distinct Tokens: 3\nDistinct Pairs: 6\nDistinct Pairs: 6\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('self', 'thy'),\n",
              " ('self', 'foe'),\n",
              " ('thy', 'self'),\n",
              " ('thy', 'foe'),\n",
              " ('foe', 'self'),\n",
              " ('foe', 'thy')]"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3of7ltFENfJr",
        "tags": []
      },
      "source": [
        "# this imports the SimpleTokenize function from the simple_tokenize.py file that you uploaded\n",
        "from simple_tokenize import simple_tokenize\n",
        "# the log function for computing PMI\n",
        "# for the sake of consistency across solutions, please use log base 10\n",
        "from math import log10\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "\n",
        "def get_pmi_analysis(file, token_1, token_2=None, threshold=None):\n",
        "    # Now, let's tokenize Shakespeare's plays\n",
        "    line_count = 0\n",
        "    token_1_count = 0\n",
        "    token_2_count = 0\n",
        "    tokens_count = 0\n",
        "    tokens = []\n",
        "\n",
        "    if token_2:\n",
        "\n",
        "        with open(file) as f:\n",
        "            for line in f:\n",
        "                # tokenize, one line at a time\n",
        "                t = simple_tokenize(line)\n",
        "\n",
        "                # Count\n",
        "                if token_1 in t:\n",
        "                    token_1_count += 1\n",
        "\n",
        "                if token_2 in t:\n",
        "                    token_2_count += 1\n",
        "\n",
        "                if token_1 in t and token_2 in t:\n",
        "                    tokens_count += 1\n",
        "\n",
        "                line_count += 1\n",
        "                tokens += set(t)\n",
        "        \n",
        "        # Calculate Probability\n",
        "        p_1 = token_1_count / line_count\n",
        "        p_2 = token_2_count / line_count\n",
        "        p_1_2 = tokens_count / line_count\n",
        "\n",
        "        # Calculate PMI\n",
        "        try:\n",
        "            pmi = log10(p_1_2 / (p_1*p_2))\n",
        "        except Exception as e:\n",
        "            pmi = np.NINF\n",
        "\n",
        "        return tokens_count, pmi \n",
        "\n",
        "    else:\n",
        "        line_count = 0\n",
        "        kv_count_dict = defaultdict(int)\n",
        "\n",
        "        with open(file) as f:\n",
        "            for line in f:\n",
        "\n",
        "                line_count+=1\n",
        "                # tokenize, one line at a time\n",
        "                t = list(set(simple_tokenize(line)))\n",
        "\n",
        "                # Add counts to the dictionary\n",
        "                if token_1 in t:\n",
        "                    kv_count_dict[token_1] += 1\n",
        "                    t.remove(token_1)\n",
        "                \n",
        "                    for other in t:\n",
        "                        kv_count_dict[token_1 + \",\" + other] += 1 # \",\" indicates combined events\n",
        "\n",
        "                for other in t:\n",
        "                        kv_count_dict[other] += 1\n",
        "        \n",
        "        # Get a list of relevant other tokens\n",
        "        combined_dict = {k:v for k, v in kv_count_dict.items() if \",\" in k}\n",
        "        other_relavant_tokens = [k.split(',')[1] for k, v in combined_dict.items() if v>=threshold]\n",
        "\n",
        "        # Calculate probability of independent and combined events\n",
        "        p_dict = {k:v/line_count for k, v in kv_count_dict.items()}\n",
        "\n",
        "        # Calculate PMI for relevant tokens satisfying threshold requirement\n",
        "        pmi_dict = defaultdict(int)\n",
        "\n",
        "        for other in other_relavant_tokens:\n",
        "            pmi_dict[other] = [log10(p_dict[token_1 + \",\" + other] / (p_dict[token_1]*p_dict[other])),\n",
        "                                kv_count_dict[token_1 + \",\" + other]]\n",
        "            \n",
        "        # Sort and Filter\n",
        "        pmi = sorted(pmi_dict.items(), key=lambda kv: kv[1][0], reverse=True)\n",
        "        \n",
        "        if len(pmi)>5:\n",
        "            pmi = pmi[:5]\n",
        "\n",
        "        return kv_count_dict[token_1], pmi\n",
        "\n",
        "###################################################################################################################\n",
        "\n",
        "###################################################################################################################\n",
        "#  the user interface below defines the types of PMI queries that users can ask\n",
        "#  you will need to modify it - where indicated - to access the results of your PMI analysis (above)\n",
        "#  so that the queries can be answered\n",
        "###################################################################################################################\n",
        "\n",
        "while True:\n",
        "    q = input(\"Input 1 or 2 space-separated tokens (return to quit): \")\n",
        "    if len(q) == 0:\n",
        "        break\n",
        "    q_tokens = simple_tokenize(q)\n",
        "    if len(q_tokens) == 1:\n",
        "        threshold = 0\n",
        "        while threshold <= 0:\n",
        "            try:\n",
        "                threshold = int(input(\"Input a positive integer frequency threshold: \"))\n",
        "            except ValueError:\n",
        "                print(\"Threshold must be a positive integer!\")\n",
        "                continue\n",
        "        \n",
        "        # Put code here to answer a One-Token Query with token q_tokens[0] and the specified threshold,\n",
        "        # and output the result.\n",
        "\n",
        "        token_count, pmi = get_pmi_analysis('Shakespeare.txt', q_tokens[0], threshold=threshold)\n",
        "        print(\"  n({0}) = {1}\".format(q_tokens[0], token_count))\n",
        "        print(\"  high PMI tokens with respect to {0} (threshold: {1}):\".format(q_tokens[0],threshold))\n",
        "\n",
        "        for output in pmi:\n",
        "            print(\"    n({0},{1}) = {2},  PMI({0},{1}) = {3}\".format(output[0], q_tokens[0], output[1][1], output[1][0]))    \n",
        "    elif len(q_tokens) == 2:\n",
        "        # Two-Token Query with tokens q_tokens[0] and q_tokens[1]\n",
        "        tokens_count, pmi = get_pmi_analysis('Shakespeare.txt', q_tokens[0], q_tokens[1])\n",
        "        print(\"  n({0},{1}) = {2}\".format(q_tokens[0],q_tokens[1],tokens_count))\n",
        "        print(\"  PMI({0},{1}) = {2}\".format(q_tokens[0],q_tokens[1],pmi))\n",
        "    else:\n",
        "        print(\"Input must consist of 1 or 2 space-separated tokens!\") \n",
        "\n",
        "# I have used log10 instead of the default log which is actually the natural log"
      ],
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  n(perfect) = 54\n  high PMI tokens with respect to perfect (threshold: 10):\n    n(in,perfect) = 11,  PMI(in,perfect) = 0.3711070042455013\n    n(i,perfect) = 12,  PMI(i,perfect) = 0.16393283689508215\n    n(and,perfect) = 14,  PMI(and,perfect) = 0.11071571970717087\n    n(the,perfect) = 10,  PMI(the,perfect) = -0.030012871217669154\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SVhvS1tNfJu"
      },
      "source": [
        "## Note\n",
        "\n",
        "The program's execution will become slower and slower as the file size increases since more lines will have to be read and/or the size of a single line may increase as well depending on the input. In both the situations the program will take more time.\n",
        "\n",
        "Furthermore, there will come a point where the program will crash since all the variables are stored in RAM of the local computer which is limited in size. For example, with a bigger file size, the size of my count dictionary `kv_count_dict` (among others) will increase signficantly such that the RAM won't be able to hold its value and eventually the program will crash."
      ]
    }
  ]
}