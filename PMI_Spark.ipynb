{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNwlw_9ObJxP"
      },
      "source": [
        "## CS431/631 Data Intensive Distributed Analytics\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lh5NCoc8fsSO"
      },
      "source": [
        "!apt-get update -qq > /dev/null\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://downloads.apache.org/spark/spark-2.4.7/spark-2.4.7-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.7-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEtWJXrebsdE"
      },
      "source": [
        "To use Spark from within a Python program, it is first necessary to tell the Python interpreter where the Spark installation is located. The code in the following cell tells Python how to find this Spark installation. This code creates SparkContext (sc) for you."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xnxe-BhPmbBW"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.7-bin-hadoop2.7\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "spark_conf = SparkConf()\\\n",
        "  .setAppName(\"YourTest\")\\\n",
        "  .setMaster(\"local[*]\")\n",
        "\n",
        "sc = SparkContext.getOrCreate(spark_conf)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wk4QOfCtbJxe"
      },
      "source": [
        "Once Python knows where Spark is located, you can create a `SparkContext`.   All Spark commands must run within an active `SparkContext`.   The code below will create a `SparkContext`, and store a reference to the context in the variable `sc`. \n",
        "The `appName` parameter assigns a name of your choosing to the Spark jobs that are created in this context - this is useful mostly for debugging.   The `master` parameter indicates that Spark jobs will run in local mode, using two threads.   This means that your Spark jobs are not really running on a cluster, and are instead running in a single process on the local machine.   You program Spark jobs the same way whether they run in local mode or on a cluster - the main difference between local and cluster modes is, of course, performance.\n",
        "\n",
        "Next, let's test that your `SparkContext` has been set up properly by running some simple test code.   This code uses a single Spark job to estimate the value of Euler's number $e$. One way to calculate $e$ is to use the following serries by Jacob Bernoulli:\n",
        "\n",
        "$p_n = 1 - \\frac{1}{1!} + \\frac{1}{2!} - \\frac{1}{3!} + \\cdots + \\frac{(-1)^n}{n!} = \\sum_{k = 0}^n \\frac{(-1)^k}{k!}$\n",
        "\n",
        "As n tends to infinity, $p_n$ approaches $1/e$.\n",
        "\n",
        "In the following code,  `parallelize()` and `map()` are Spark *transformations*, and `reduce()` is a Spark *action*.   Study the code in the cell below, then go ahead and run it.   It should take several seconds, since a Spark job is being created and executed, and should print an estimate of $e$ when it finishes.   \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "octW5eVWbJxg",
        "outputId": "cb2c340b-c253-47d4-c6cc-fbea3272a1e1"
      },
      "source": [
        "import math\n",
        "\n",
        "n= 10000\n",
        "inverse_e = sc.parallelize(range(0, n)).map(lambda x: ((-1)**x) * (1 / math.factorial(x))).reduce(lambda x,y:x+y)\n",
        "e = 1 / inverse_e\n",
        "print(\"e = \", e)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "e =  2.718281828459044\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBiT9mjQbJxh"
      },
      "source": [
        "It starts off by making use of the SparkContext (sc) which tells Python that we are looking for spark functionality implemented in the SparkContext class. The python range (0, n) is then parallized, meaning, it is \"transformed\" to an RDD which is a distributed collection of objects in Spark whose day is partitioned across multiple clusters (single node in this stand-alone case). Next, the RDD is further transformed (giving as another RDD) using the map operation which in this particular example, converts the integers to individual terms of the given series. It is worth mentioning that the RDD has not executed any transformation as of now. A spark program maintains the order of execution in the form of a DAG (Directed Acyclic Graph) where each vertex represent an RDD and each preceding edge represent the operation that produced that RDD. In other words, each RDD maintains a pointer to it's parent RDD that maintains the transformation that produced it. This is known as lineage.\n",
        "\n",
        "Coming back to this program, the mapped RDD is then reduced to a single value by calculating a running sum of each of the consecutive terms one by one, summing all the terms of the given series. This action of reduce() is what submits the DAG to the DAG scheduler in Spark and leads to calculation of the specified RDDs. Finally, we take the reciprocal of the series using Python to estimate the value of e. The accuracy of e depends on the chosen value of n. The larger the value of n, the more accurate the estimate will become."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcXDTtuqENqF"
      },
      "source": [
        "!wget -q https://student.cs.uwaterloo.ca/~cs451/content/cs431/Shakespeare.txt\n",
        "!wget -q https://student.cs.uwaterloo.ca/~cs451/content/cs431/simple_tokenize.py"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYckA9JK8feF"
      },
      "source": [
        "file = \"Shakespeare.txt\""
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuFxV3z8bJxi"
      },
      "source": [
        "from simple_tokenize import simple_tokenize\n",
        "\n",
        "# Returns the count of distinct tokens in the `Shakespeare.txt` dataset\n",
        "def count_distinct_tokens(sc, file):\n",
        "    # your solution to Question 2 here\n",
        "\n",
        "    text_file = sc.textFile(file)\n",
        "\n",
        "    # Works but not efficient as all the data moved on the network.\n",
        "    # text_take = text_file.flatMap(lambda x: simple_tokenize(x)).map(lambda x: (x,1)).groupByKey().mapValues(sum).count()\n",
        "\n",
        "    # Works and is efficient because of combinining although we don't want sum for now.\n",
        "    text_take = text_file.flatMap(lambda x: simple_tokenize(x)).map(lambda x: (x,1)).reduceByKey(lambda x,y: x+y).count()\n",
        "    return text_take"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V3eitDUUO4gZ",
        "outputId": "cb0f42e6-ae3d-4266-b945-6e24b1938a0f"
      },
      "source": [
        "count_distinct_tokens(sc, file)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25975"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n43vVrAebJxk"
      },
      "source": [
        "from simple_tokenize import simple_tokenize\n",
        "\n",
        "# Returns the count of distinct pairs in the `Shakespeare.txt` dataset\n",
        "from itertools import permutations\n",
        "\n",
        "def make_pairs(x_list):\n",
        "  \"This function returns unique permutations excluding out reflexive pairs.\"\n",
        "  arr = set(x_list)\n",
        "  pairs = []\n",
        "  for first_token in arr:\n",
        "    for second_token in arr:\n",
        "      if first_token != second_token:\n",
        "        pairs.append((first_token, second_token))\n",
        "  return pairs\n",
        "\n",
        "def count_distinct_pairs(sc, file):\n",
        "    # your solution to Question 3 here\n",
        "    with open(file) as f:\n",
        "      text_file = sc.parallelize(f)\n",
        "      take = text_file.map(lambda x: simple_tokenize(x)).flatMap(lambda x: make_pairs(x)).map(lambda x: (x,1)).groupByKey().mapValues(sum).count()\n",
        "    return take"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkmTZei_RiAi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "177bc89d-24ab-441c-9a3d-5ebd6c956794"
      },
      "source": [
        "count_distinct_pairs(sc, file)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1969760"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmKHDBW9bJxl"
      },
      "source": [
        "from simple_tokenize import simple_tokenize\n",
        "\n",
        "def no_rep(x):\n",
        "  \"Since we count number of lines instead of actual occurrences of a token, we remove duplicates.\"\n",
        "  return list(set(x))\n",
        "\n",
        "# Returns a list of the top 50 (probability, count, token) tuples, ordered by probability\n",
        "def top_50_tokens_probabilities(sc, file):\n",
        "    # your solution to Question 4 here \n",
        "    with open(file) as f:\n",
        "      text_file = sc.textFile(file) \n",
        "      text_file.cache() # what if cache before action\n",
        "      lines_count = text_file.count()\n",
        "      tokens_count = text_file.map(lambda x: simple_tokenize(x)).flatMap(lambda x: no_rep(x)).map(lambda x: (x,1)).reduceByKey(lambda x,y: x+y).map(lambda x: (x[1], x[0])).sortByKey(False).map(lambda x: (x[1], x[0]))\n",
        "      tokens_count = tokens_count.map(lambda x: (x[0], x[1], x[1] / lines_count)).take(50)\n",
        "      \n",
        "    return tokens_count"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SLtcTOa0HrTK",
        "outputId": "80c320e7-c676-48ab-f06a-eee8e4662ea9"
      },
      "source": [
        "top_50_tokens_probabilities(sc, file)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('and', 24604, 0.2009178657172255),\n",
              " ('the', 24300, 0.19843538192686472),\n",
              " ('i', 18657, 0.1523542765682928),\n",
              " ('to', 18237, 0.148924529226347),\n",
              " ('of', 16624, 0.1357526662202551),\n",
              " ('a', 13280, 0.10844534452628657),\n",
              " ('you', 12196, 0.09959332995802643),\n",
              " ('my', 11549, 0.09430988583840991),\n",
              " ('in', 10614, 0.08667461497003054),\n",
              " ('that', 10569, 0.08630714204053634),\n",
              " ('is', 8756, 0.07150206601447026),\n",
              " ('not', 8230, 0.06720671577193814),\n",
              " ('with', 7552, 0.06167012363422561),\n",
              " ('me', 7396, 0.06039621747864574),\n",
              " ('for', 7326, 0.05982459292165477),\n",
              " ('it', 7147, 0.05836286726877787),\n",
              " ('be', 6662, 0.054402325695340446),\n",
              " ('this', 6425, 0.05246696826667102),\n",
              " ('his', 6403, 0.05228731483447386),\n",
              " ('your', 6233, 0.050899083767495794),\n",
              " ('but', 6205, 0.05067043394469941),\n",
              " ('he', 5816, 0.0474938346208496),\n",
              " ('have', 5742, 0.046889545803459144),\n",
              " ('thou', 5060, 0.04132028940534714),\n",
              " ('as', 4917, 0.04015254209606559),\n",
              " ('him', 4840, 0.03952375508337552),\n",
              " ('so', 4836, 0.03949109082297604),\n",
              " ('will', 4831, 0.039450260497476686),\n",
              " ('what', 4305, 0.03515491025494455),\n",
              " ('all', 3824, 0.031227032941906614),\n",
              " ('thy', 3751, 0.030630910189616032),\n",
              " ('by', 3639, 0.029716310898430482),\n",
              " ('do', 3626, 0.02961015205213216),\n",
              " ('no', 3549, 0.028981365039442094),\n",
              " ('shall', 3534, 0.028858874062944028),\n",
              " ('her', 3454, 0.02820558885495435),\n",
              " ('if', 3449, 0.028164758529454995),\n",
              " ('are', 3296, 0.026915350569174736),\n",
              " ('we', 3125, 0.0255189534370968),\n",
              " ('thee', 3048, 0.024890166424406734),\n",
              " ('on', 2975, 0.024294043672116156),\n",
              " ('lord', 2943, 0.024032729588920283),\n",
              " ('our', 2890, 0.023599928138627123),\n",
              " ('king', 2810, 0.022946642930637442),\n",
              " ('good', 2727, 0.022268859527348153),\n",
              " ('now', 2724, 0.02224436133204854),\n",
              " ('sir', 2629, 0.021468585147560795),\n",
              " ('from', 2595, 0.021190938934165182),\n",
              " ('o', 2532, 0.02067647683287331),\n",
              " ('at', 2461, 0.020096686210782474)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZMY3pI9bJxm"
      },
      "source": [
        "from simple_tokenize import simple_tokenize\n",
        "from math import log10\n",
        "\n",
        "# Returns a list of tuples with the following format:\n",
        "# ((token1, token2), pmi, co-occurrence_count, token1_count, token2_count)\n",
        "def PMI(sc, file, threshold, reduce=True):\n",
        "    # your solution to Question 5 here\n",
        "    with open(file) as f:\n",
        "      text_file = sc.textFile(file) \n",
        "\n",
        "      # cache-ing so text_file is not \"re\"-loaded while calculating \"single token count and probability\". \n",
        "      text_file.cache() \n",
        "      lines_count = text_file.count()\n",
        "\n",
        "      # single token count and probability\n",
        "      tokens_count = text_file.map(lambda x: simple_tokenize(x)).flatMap(lambda x: no_rep(x)).map(lambda x: (x,1)).reduceByKey(lambda x,y: x+y).map(lambda x: (x[1], x[0])).sortByKey(False).map(lambda x: (x[1], x[0]))\n",
        "      tokens_count = tokens_count.map(lambda x: (x[0], (x[1], x[1] / lines_count)))\n",
        "\n",
        "      # making pairs of tokens\n",
        "      pairs = text_file.map(lambda x: simple_tokenize(x)).flatMap(lambda x: make_pairs(x)).map(lambda x: (x,1)).groupByKey().mapValues(sum).filter(lambda x: x[1] >= threshold)\n",
        "      \n",
        "      # reformatting so information for first token of a pair can be joined\n",
        "      first_join_prep = pairs.map(lambda x: (x[0][0], list([x[0][1], x[1], x[1]/lines_count])))\n",
        "      first_joined = first_join_prep.join(tokens_count)\n",
        "\n",
        "      # reformatting for the second token and joining information\n",
        "      second_join_prep = first_joined.map(lambda x: (x[1][0][0], [x[1][0][1], x[1][0][2], x[0], x[1][1][0], x[1][1][1]]))\n",
        "      second_joined = second_join_prep.join(tokens_count)\n",
        "\n",
        "      # reformatting and calculating PMI\n",
        "      final = second_joined.map(lambda x: ((x[1][0][2], x[0]), [x[1][0][0], x[1][0][1], x[1][0][3], x[1][0][4], x[1][1][0], x[1][1][1]]))\n",
        "      final_pmi = final.map(lambda x: (x[0], log10(x[1][1] / (x[1][3] * x[1][5])), x[1][0], x[1][2], x[1][4]))\n",
        "\n",
        "      if reduce:\n",
        "        return final_pmi.collect()\n",
        "      else:\n",
        "        return final_pmi"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lu9Ph_YL7lU8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4be5353-2572-4bec-e6b8-16d69ec8fc8e"
      },
      "source": [
        "PMI(sc, file, 6000)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(('the', 'of'), 0.342940751918893, 7266, 24300, 16624),\n",
              " (('of', 'the'), 0.342940751918893, 7266, 16624, 24300)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUrjYZm6bJxn"
      },
      "source": [
        "from simple_tokenize import simple_tokenize\n",
        "from math import log\n",
        "\n",
        "# Returns a list of samp_size tuples with the following format:\n",
        "# (token, [ list_of_cooccurring_tokens ])\n",
        "# where list_of_cooccurring_tokens is of the form\n",
        "# [((token1, token2), pmi, cooc_count, token1_count, token2_count), ...]\n",
        "def PMI_one_token(sc, file, threshold, samp_size):\n",
        "    # your solution to Question 6 here\n",
        "\n",
        "    # calculating PMIs for distinct tokens based on threshold\n",
        "    pmi = PMI(sc, file, threshold, reduce=False)\n",
        "\n",
        "    # grouping the PMIs by a single token (the first token)\n",
        "    pmi_one_token = pmi.map(lambda x: (x[0][0], x)).groupByKey().map(lambda x: (x[0], list(x[1])))\n",
        "    \n",
        "    # sampling\n",
        "    return pmi_one_token.takeSample(withReplacement=False, num=samp_size, seed=1)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lRdWS45DqT-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23c5f0e5-b057-426d-cbcd-8c5a6f7778c8"
      },
      "source": [
        "PMI_one_token(sc, file, threshold=2000, samp_size=10)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('my',\n",
              "  [(('my', 'and'), 0.0056896916303759305, 2351, 11549, 24604),\n",
              "   (('my', 'i'), 0.16722948986833625, 2586, 11549, 18657)]),\n",
              " ('will', [(('will', 'i'), 0.45697891972441507, 2108, 4831, 18657)]),\n",
              " ('of',\n",
              "  [(('of', 'and'), 0.028305447826683598, 3565, 16624, 24604),\n",
              "   (('of', 'the'), 0.342940751918893, 7266, 16624, 24300),\n",
              "   (('of', 'i'), -0.08531809933104956, 2081, 16624, 18657),\n",
              "   (('of', 'a'), 0.13551796879761382, 2463, 16624, 13280)]),\n",
              " ('a',\n",
              "  [(('a', 'of'), 0.13551796879761382, 2463, 13280, 16624),\n",
              "   (('a', 'and'), 0.0006198226234107662, 2672, 13280, 24604),\n",
              "   (('a', 'to'), 0.048227962752620035, 2210, 13280, 18237),\n",
              "   (('a', 'i'), 0.02456394293286546, 2141, 13280, 18657)]),\n",
              " ('in',\n",
              "  [(('in', 'and'), 0.04031821796867754, 2340, 10614, 24604),\n",
              "   (('in', 'the'), 0.13332315333353123, 2863, 10614, 24300)]),\n",
              " ('that',\n",
              "  [(('that', 'the'), 0.069458604407197, 2461, 10569, 24300),\n",
              "   (('that', 'i'), 0.11221751207929728, 2085, 10569, 18657)]),\n",
              " ('to',\n",
              "  [(('to', 'and'), 0.017522588038697155, 3815, 18237, 24604),\n",
              "   (('to', 'you'), 0.1276718775908892, 2437, 18237, 12196),\n",
              "   (('to', 'the'), 0.05123525982989819, 4072, 18237, 24300),\n",
              "   (('to', 'i'), 0.046852605922459045, 3095, 18237, 18657),\n",
              "   (('to', 'a'), 0.048227962752620035, 2210, 18237, 13280)]),\n",
              " ('the',\n",
              "  [(('the', 'of'), 0.342940751918893, 7266, 24300, 16624),\n",
              "   (('the', 'that'), 0.069458604407197, 2461, 24300, 10569),\n",
              "   (('the', 'and'), 0.0459349918330654, 5427, 24300, 24604),\n",
              "   (('the', 'to'), 0.05123525982989819, 4072, 24300, 18237),\n",
              "   (('the', 'in'), 0.13332315333353123, 2863, 24300, 10614),\n",
              "   (('the', 'i'), -0.08302344185158504, 3058, 24300, 18657),\n",
              "   (('the', 'is'), 0.14639570960345155, 2434, 24300, 8756)]),\n",
              " ('am', [(('am', 'i'), 0.8031984391122662, 2053, 2120, 18657)]),\n",
              " ('i',\n",
              "  [(('i', 'of'), -0.08531809933104956, 2081, 18657, 16624),\n",
              "   (('i', 'have'), 0.44724824687447107, 2450, 18657, 5742),\n",
              "   (('i', 'that'), 0.11221751207929728, 2085, 18657, 10569),\n",
              "   (('i', 'and'), -0.050374035338056455, 3338, 18657, 24604),\n",
              "   (('i', 'to'), 0.046852605922459045, 3095, 18657, 18237),\n",
              "   (('i', 'you'), 0.18638190375571081, 2854, 18657, 12196),\n",
              "   (('i', 'am'), 0.8031984391122662, 2053, 18657, 2120),\n",
              "   (('i', 'the'), -0.08302344185158504, 3058, 18657, 24300),\n",
              "   (('i', 'not'), 0.2812323291754297, 2396, 18657, 8230),\n",
              "   (('i', 'will'), 0.45697891972441507, 2108, 18657, 4831),\n",
              "   (('i', 'my'), 0.16722948986833625, 2586, 18657, 11549),\n",
              "   (('i', 'a'), 0.02456394293286546, 2141, 18657, 13280)])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzE7ys9yGei-"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}